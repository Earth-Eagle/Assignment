{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Setup (Colab)\n",
        "\n",
        "# Importieren von Colab-Hilfsfunktionen, um sicher gespeicherte Userdaten (z. B.\n",
        "# API-Keys) abzurufen, ohne sie im Code zu hinterlegen.\n",
        "from google.colab import userdata\n",
        "\n",
        "#Auslesen des gespeicherten OpenAI-API-Schlüssels aus Colab Secrets.\n",
        "OPENAI_API_KEY = userdata.get('apikey_lp')"
      ],
      "metadata": {
        "id": "DdIw28ohHahv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pakete\n",
        "\n",
        "# LangChain-Kerne, Community-Integrationen, das OpenAI-Plugin, PDF-Parsing sowie\n",
        "# FAISS für den lokalen Vektorindex installieren.\n",
        "!pip install -q langchain langchain-community langchain-openai pypdf faiss-cpu"
      ],
      "metadata": {
        "id": "EDdMUfJkTVyU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "# LangChain-Wrapper: ChatOpenAI kapselt Chat-Completions der OpenAI-API,\n",
        "# OpenAIEmbeddings kapselt Embedding-Calls.\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Lädt PDFs über pypdf und abstrahiert das Seiten-Parsing in\n",
        "# LangChain-Dokument-Objekte.\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Zerlegt lange Texte rekursiv in überlappende Chunks, um semantisches Retrieval\n",
        "# stabil zu machen.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Stellt eine LangChain-Hülle um den FAISS-Vektorindex bereit, wodurch Einfügen\n",
        "# und Ähnlichkeitssuche vereinheitlicht werden.\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Parst die LLM-Ausgabe und extrahiert den reinen Antwort-String aus der\n",
        "# generischen Nachrichtenstruktur.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Erzeugt Parameterisierbare Prompt-Schablonen, die Variablen wie {context} und\n",
        "# {question} zur Laufzeit einsetzen.\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Bausteine der LangChain-Execution-Engine: Parallel führt mehrere Schritte\n",
        "# gleichzeitig aus, Passthrough reicht Eingaben unverändert weiter.\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
      ],
      "metadata": {
        "id": "7wknJowZTX-T"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF laden\n",
        "\n",
        "# Definieren der Quelle der PDF als Rohdatei-URL auf GitHub, damit der Loader\n",
        "# direkt herunterladen kann.\n",
        "PDF_PATH = \"https://raw.githubusercontent.com/Earth-Eagle/Assignment/main/WissensquelleRAG.pdf\"\n",
        "\n",
        "# Dokument-Loader erstellen, der intern pypdf nutzt und jede Seite als eigenes\n",
        "# Document-Objekt abstrahiert.\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "\n",
        "# PDF synchron laden, liefert eine Liste (ein Element pro Seite).\n",
        "docs = loader.load()\n",
        "\n",
        "# Seitenzahl auf 10 beschränken (sicherstellen dass max. 10 Seiten genutzt\n",
        "# werden, wie in der Aufgabenstellung gewünscht).\n",
        "docs = docs[:10]"
      ],
      "metadata": {
        "id": "GRd55oR_T7Pb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking\n",
        "\n",
        "# Rekursiven Text-Splitter konfigurieren, der nach Grenzzeichen teilt.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "\n",
        "   # Maximale Zeichenlänge eines Chunks festlegen.\n",
        "    chunk_size=800,\n",
        "\n",
        "   # Überlappung zwischen Chunks definieren um semantische Kontinuität zu\n",
        "   # sichern.\n",
        "    chunk_overlap=120\n",
        ")\n",
        "\n",
        "# Splitter auf die Dokumentenliste anwenden --> Chunks mit vererbten Metadaten\n",
        "# werden produziert.\n",
        "chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "s2Sky7FZT1Oc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings & Vectorstore (semantisches Retrieval)\n",
        "\n",
        "# Embedding-Client erstellen, der HTTP-Aufrufe zur OpenAI-Embeddings-API\n",
        "# kapselt und Texte in numerische Vektoren umwandelt.\n",
        "embeddings = OpenAIEmbeddings(\n",
        "\n",
        "    # Embedding Modell auswählen.\n",
        "    model=\"text-embedding-3-small\",\n",
        "\n",
        "    # API-Schlüssel übergeben damit Wrapper Requests an OPENAI-API senden kann.\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")\n",
        "\n",
        "# Embedding für alle Chunks berechnen und FAISS-Index für Approximate Nearest\n",
        "# Neighbor Suche bauen.\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "# Vectorstore in Retriever-Interface umwandeln, das intern k ähnlichste Chunks\n",
        "# liefert.\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "r5lZCQwmTy5X"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt + LLM + Parser (RAG-Chain)\n",
        "\n",
        "# Chat-Promptvorlage erstellen.\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Beantworte die Frage NUR auf Basis des bereitgestellten Kontexts.\n",
        "Wenn die Antwort nicht eindeutig im Kontext steht, gib EXAKT: \"Ich weiß es nicht.\" zurück.\n",
        "Antworte kurz und präzise auf Deutsch.\n",
        "\n",
        "# Kontext\n",
        "{context}\n",
        "\n",
        "# Frage\n",
        "{question}\"\"\"\n",
        ")\n",
        "\n",
        "# LLM-Client erzeugen\n",
        "model = ChatOpenAI(\n",
        "\n",
        "    # API-Schlüssel für authentifizierte Requests setzen,\n",
        "    # die über das Wrapper-Objekt abgesetzt werden.\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "\n",
        "    # Chatmodell auswählen.\n",
        "    model=\"gpt-4o-mini\",\n",
        "\n",
        "    # Temperature festlegen, um Zufälligkeit der Ausgabe zu regeln.\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Parser festlegen, der die LLM-Antwort in einen String extrahiert.\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "Pa7Gh7xwTxUQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frage durchreichen und Kontext aus Retriever ziehen\n",
        "\n",
        "# Parallelen Ausführungsknoten erstellen, der mehrere Runnables gleichzeitig\n",
        "# berechnet.\n",
        "setup = RunnableParallel(\n",
        "\n",
        "    # context mit den vom Retriever zur Frage gefundenen passenden\n",
        "    # Textstellen befüllen.\n",
        "    context=retriever,\n",
        "\n",
        "    # Eingabe unverändert weiterreichen.\n",
        "    question=RunnablePassthrough()\n",
        ")"
      ],
      "metadata": {
        "id": "3ECngiIKTsEQ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gesamtkette: (Retriever) -> Prompt -> LLM -> Parser\n",
        "\n",
        "# Runnables mit Pipe-Operator verknüpfen, sodass Ausgabe eines Schrittes\n",
        "# zum Eingang des nächsten wird.\n",
        "rag_chain = setup | prompt | model | parser\n"
      ],
      "metadata": {
        "id": "-tJGMV-ATq0D"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispielabfragen\n",
        "\n",
        "# Antwort auf die Frage ausgeben.\n",
        "print(\n",
        "\n",
        "    # Kette synchron ausführen: Retrieval erzeugt Kontext, Prompt wird\n",
        "    # gerendert, Modell antwortet, Parser gibt String zurück.\n",
        "    rag_chain.invoke(\"Worum geht es in dem Dokument?\")\n",
        ")\n",
        "\n",
        "print(\n",
        "    rag_chain.invoke(\"Welche Rivalen hatte Alonso?\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgF247peTpnD",
        "outputId": "7cc5e4cb-6660-436e-864a-0f9db7fb14bb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Das Dokument behandelt die Karriere und Erfolge des Rennfahrers Fernando Alonso, einschließlich seiner Zeit bei Ferrari und Aston Martin sowie seiner Beziehungen und persönlichen Interessen.\n",
            "Alonso hatte Rivalen wie Michael Schumacher, Kimi Räikkönen und Lewis Hamilton.\n"
          ]
        }
      ]
    }
  ]
}