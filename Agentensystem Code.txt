# Bibliotheken installieren
# Langchain und Langchain-openai f√ºr LLM-Workflows, Tool-Integration 
# und Prompt-Chaining
# requests f√ºr HTTP-Anfragen (Wetter-API)
# pandas f√ºr Tabellen-Output
!pip install langchain langchain-openai requests pandas

# Keys aus Colab-Secret-Store laden
from google.colab import userdata
import os

# API-Keys aus userdata holen
# OpenAI-Key
os.environ["OPENAI_API_KEY"] = userdata.get("apikey_lp")
# OpenWeatherMap-Key
os.environ["OPENWEATHER_API_KEY"] = userdata.get("wetterkey_lp")


# Imports

# Langchain-Schnittstelle zu OpenAI-ChatModell
from langchain_openai import ChatOpenAI

#ChatPromptTemplate f√ºr Templates mit Variablen
from langchain.prompts import ChatPromptTemplate

# F√ºr direkten API-Zugriff
import requests

# JSON Import f√ºr structued output
import json

# Pandas f√ºr Tabellenanzeige
import pandas as pd

# Arbeiten mit regul√§ren Ausdr√ºcken
import re

# Wetter-Tool (OpenWeatherMap)

# Hier werden mithilfe einer API Wetterdaten f√ºr eine Stadt abgerufen.
# Stellt ein Tool dar, dessen Output f√ºr das Prompt-Chaining genutzt wird
def get_weather(city):
    api_key = os.environ["OPENWEATHER_API_KEY"]
    # API-Endpoint mit Parametern: Stadt, API-Key, metrische Einheiten, deutsche Sprache
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric&lang=de"
    
    # Anfrage an die Wetter-API
    resp = requests.get(url)
    data = resp.json()
    
    # Falls Statuscode nicht 200 ‚Üí API-Fehler zur√ºckgeben
    if resp.status_code != 200:
        return {"error": data.get("message", "Fehler beim Abrufen")}
    
    # Wichtigste Felder ins Dictionary √ºbernehmen
    return {
        "stadt": data["name"],
        "temperatur": data["main"]["temp"],
        "wetterbeschreibung": data["weather"][0]["description"]
    }

# Prompt-Chaining

# Mehrstufiger Ablauf:
# 1) Tool-Aufruf: get_weather() ‚Üí externe Daten holen
# 2) LLM-Aufruf: ChatOpenAI mit Wetterdaten als Input ‚Üí Empfehlung generieren
# 3) Structured Output: JSON-Ergebnis parsen und zur√ºckgeben
def chain(city):

    # Wetter abrufen
    wetter = get_weather(city)

    if "error" in wetter:
        return wetter # Fehler durchreichen

    # Empfehlung mit LLM
    # ChatOpenAI ist LLM-Wrapper, gpt-4o-mini mit Temperatur 0 wird verwendet
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    # Mit ChatPromptTemplate k√∂nnen Vorlage erstellen um Variablen gut einsetzen zu k√∂nnen
    prompt = ChatPromptTemplate.from_template("""
    Du bist ein Wetterassistent. Analysiere die folgenden Wetterdaten und gib eine Aktivit√§tsempfehlung.
    Antworte im JSON-Format mit den Feldern: stadt, temperatur, wetterbeschreibung, empfehlung.
    Wetterdaten: {wetter}
    """)

    # Erzeugung einer Chain, erst wird Prompt bef√ºllt, dann automatisch an LLM geschickt
    chain = prompt | llm

    # Chain ausf√ºhren, Ergebnis abfragen
    result = chain.invoke({"wetter": wetter})

    # Rohtextantwort bereinigen
    text = result.content.strip()
    
    # Codebl√∂cke entfernen (```json ... ```)
    if text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z0-9]*\n?", "", text)  # entfernt ```json
        text = re.sub(r"```$", "", text).strip()

    # Text als JSON laden
    try:
        return json.loads(text)
    except json.JSONDecodeError:
      # Rohtext zur√ºckgeben wenn Parsing fehlschl√§gt
        return {"rohtext": text}

# Testlauf und Tabellen-Output

# Stadt festlegen
stadt = "Heidenheim"

# Prompt-Chaining-Funktion ausf√ºhren
ergebnis = chain(stadt)

# JSON sch√∂n ausgeben
print("JSON-Output:")
print(json.dumps(ergebnis, indent=2, ensure_ascii=False))

# Tabelle anzeigen, wenn m√∂glich
if isinstance(ergebnis, dict):
    df = pd.DataFrame([ergebnis])
    print("\n Tabellen-Output:")
    display(df)
else:
    print("\n Ergebnis konnte nicht in eine Tabelle umgewandelt werden.")
    # Codebl√∂cke entfernen (```json ... ```)
    if text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z0-9]*\n?", "", text)  # entfernt ```json
        text = re.sub(r"```$", "", text).strip()

    # Text als JSON laden
    try:
        return json.loads(text)
    except json.JSONDecodeError:
      # Rohtext zur√ºckgeben wenn Parsing fehlschl√§gt
        return {"rohtext": text}

# Testlauf und Tabellen-Output

# Stadt festlegen
stadt = "Heidenheim"

# Prompt-Chaining-Funktion ausf√ºhren
ergebnis = chain(stadt)

# JSON sch√∂n ausgeben
print("üìÑ JSON-Output:")
print(json.dumps(ergebnis, indent=2, ensure_ascii=False))

# Tabelle anzeigen, wenn m√∂glich
if isinstance(ergebnis, dict):
    df = pd.DataFrame([ergebnis])
    print("\nüìä Tabellen-Output:")
    display(df)
else:
    print("\n‚ö†Ô∏è Ergebnis konnte nicht in eine Tabelle umgewandelt werden.")
        return json.loads(text)
    except json.JSONDecodeError:
      # Rohtext zur√ºckgeben wenn Parsing fehlschl√§gt
        return {"rohtext": text}

# 6. Testlauf + Tabellen-Output

# Stadt festlegen
stadt = "Heidenheim"

# Prompt-Chaining-Funktion ausf√ºhren
ergebnis = chain(stadt)

# JSON sch√∂n ausgeben
print("üìÑ JSON-Output:")
print(json.dumps(ergebnis, indent=2, ensure_ascii=False))

# Tabelle anzeigen, wenn m√∂glich
if isinstance(ergebnis, dict):
    df = pd.DataFrame([ergebnis])
    print("\nüìä Tabellen-Output:")
    display(df)
else:
    print("\n‚ö†Ô∏è Ergebnis konnte nicht in eine Tabelle umgewandelt werden.")